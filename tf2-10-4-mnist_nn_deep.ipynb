{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ti2E9BmCArOBakWHEduad5GeN4J1dnYU",
      "authorship_tag": "ABX9TyPS2K6nZ7j4NVHlqx51XbO2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Burgunthy/Tensorflow2_Practice/blob/main/tf2-10-4-mnist_nn_deep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT8rOGI_CtYu"
      },
      "source": [
        "# MNist를 사용하여 실습 진행\n",
        "# softmax를 이용하여 output 구함\n",
        "# layer의 크기를 바꿔가며 가장 효율적인 모델 구축\n",
        "# loss: 0.0145 - accuracy: 0.9956\n",
        "# 너무 과한 레이어 추가는 독이 된다\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kv7PfSuCuU3"
      },
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "# 한번에 100 개의 dataset 씩 학습\n",
        "training_epochs = 15\n",
        "# 15번의 training\n",
        "nb_classes = 10\n",
        "# 마지막 softmax 출력 값"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gXI1SOUCwWg",
        "outputId": "7f8b946d-e38f-4eb5-b977-99a0f67c7bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "# keras를 통해 mnist 파일 불러오기\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# MNist data load. -> 추후 개인 공부에도 사용\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "# 픽셀을 255로 나눠 0 ~ 1 사이의 값으로 normalization\n",
        "\n",
        "print(x_train.shape)  # (60000, 28, 28)\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
        "print(x_train.shape) # (60000, 784)\n",
        "# 쉬운 학습을 위해 3차원 -> 2차원 변환\n",
        "# * 을 사용하여 위와 같이 일렬로 만들 수 있다\n",
        "\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "# in tf1, one_hot= True in read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# 직접 one_hot encoding하여 train, test의 결과값을 저장한다"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W2VbvfpCzTG",
        "outputId": "8caa0ffc-e51f-44c8-9beb-7ae497a31b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "source": [
        "# Xavier 정규분포 초기값 설정\n",
        "# stddev = sqrt(2 / (fan_in + fan_out))\n",
        "\n",
        "tf.model = tf.keras.Sequential()\n",
        "tf.model.add(tf.keras.layers.Dense(input_dim=784, units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
        "tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
        "tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
        "tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
        "tf.model.add(tf.keras.layers.Dense(units=nb_classes, kernel_initializer='glorot_normal', activation='softmax'))\n",
        "# one_hot을 위해 해당 loss 사용\n",
        "\n",
        "tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
        "tf.model.summary()\n",
        "\n",
        "history = tf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n",
        "# batch와 epoch을 이와 같이 활용하니 더욱 깔끔하다.\n",
        "# pytorch보다 괜찮고 깔끔하게 사용 가능"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,195,018\n",
            "Trainable params: 1,195,018\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.2125 - accuracy: 0.9362\n",
            "Epoch 2/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0903 - accuracy: 0.9727\n",
            "Epoch 3/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0611 - accuracy: 0.9815\n",
            "Epoch 4/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0489 - accuracy: 0.9848\n",
            "Epoch 5/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0415 - accuracy: 0.9870\n",
            "Epoch 6/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9900\n",
            "Epoch 7/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0287 - accuracy: 0.9912\n",
            "Epoch 8/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0272 - accuracy: 0.9919\n",
            "Epoch 9/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0240 - accuracy: 0.9930\n",
            "Epoch 10/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0194 - accuracy: 0.9943\n",
            "Epoch 11/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9939\n",
            "Epoch 12/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9944\n",
            "Epoch 13/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9945\n",
            "Epoch 14/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0159 - accuracy: 0.9952\n",
            "Epoch 15/15\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.0145 - accuracy: 0.9956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiEHPmthC5Y-",
        "outputId": "a65282c8-6fe6-46a3-f752-f4c23dbb7873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "predictions = tf.model.predict(x_test)\n",
        "print('Prediction: \\n', predictions)\n",
        "\n",
        "score = tf.model.evaluate(x_train, y_train)\n",
        "print('Accuracy: ', score[1])\n",
        "# loss: 0.2455 - accuracy: 0.9320"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: \n",
            " [[2.5298670e-20 3.9069307e-13 1.2227354e-10 ... 1.0000000e+00\n",
            "  5.1164466e-14 1.6159697e-10]\n",
            " [6.3569628e-06 3.8819093e-05 9.9952018e-01 ... 4.7641406e-05\n",
            "  7.8538411e-05 1.0796118e-08]\n",
            " [5.8899097e-12 1.0000000e+00 8.0687812e-09 ... 4.9574629e-09\n",
            "  7.2963896e-10 6.6879614e-14]\n",
            " ...\n",
            " [1.6002577e-16 2.1789361e-12 4.0393517e-14 ... 5.5697413e-10\n",
            "  3.4446133e-09 1.6300325e-07]\n",
            " [2.3984482e-20 3.5272342e-21 6.6180853e-20 ... 1.4841829e-20\n",
            "  3.2661727e-18 2.2898776e-16]\n",
            " [2.6963769e-15 1.7427298e-18 3.9521445e-18 ... 7.2033634e-23\n",
            "  1.6857861e-15 3.8717331e-23]]\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0150 - accuracy: 0.9954\n",
            "Accuracy:  0.9954333305358887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfvK8_e8Hq0K",
        "outputId": "8af21df1-4a0f-491b-8615-c0d8b43900cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# test 하는 함수\n",
        "random.seed(777)\n",
        "\n",
        "y_predicted = tf.model.predict(x_test)\n",
        "# x_test를 통해 미리 predict된 list 생성\n",
        "for x in range(0, 10):\n",
        "    random_index = random.randint(0, x_test.shape[0]-1)\n",
        "    # x_test의 크기 중 random index를 구한다.\n",
        "    \n",
        "    print(\"index: \", random_index,\n",
        "          \"actual y: \", np.argmax(y_test[random_index]),\n",
        "          \"predicted y: \", np.argmax(y_predicted[random_index]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index:  3757 actual y:  8 predicted y:  8\n",
            "index:  7304 actual y:  5 predicted y:  5\n",
            "index:  7300 actual y:  7 predicted y:  7\n",
            "index:  6039 actual y:  9 predicted y:  9\n",
            "index:  9429 actual y:  3 predicted y:  3\n",
            "index:  4420 actual y:  5 predicted y:  5\n",
            "index:  5507 actual y:  2 predicted y:  2\n",
            "index:  8809 actual y:  1 predicted y:  1\n",
            "index:  654 actual y:  5 predicted y:  5\n",
            "index:  7302 actual y:  8 predicted y:  8\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}